import torch
import pandas as pd
import numpy as np
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os
import json
import warnings
warnings.filterwarnings('ignore')

class TestDatasetEvaluator:
    def __init__(self, model_path, test_data_path="dataset/test_labeled_dataset.csv"):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í‰ê°€ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
        """
        self.model_path = model_path
        self.test_data_path = test_data_path
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        try:
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path, trust_remote_code=True)
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ í† í¬ë‚˜ì´ì € ê²°ì •
            if 'kobert' in model_path.lower():
                self.tokenizer = AutoTokenizer.from_pretrained('monologg/kobert', trust_remote_code=True)
                print("KoBERT í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            else:
                self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                print("BERT í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            self.model.to(self.device)
            
            # ë¼ë²¨ ë§¤í•‘ ì„¤ì •
            self.label2id = {"Work": 0, "Personal": 1, "Advertisement": 2}
            self.id2label = {0: "Work", 1: "Personal", 2: "Advertisement"}
            
            print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def load_test_data(self):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        """
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤... ({self.test_data_path})")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(self.test_data_path)
        
        # ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
        df['label_id'] = df['label'].map(self.label2id)
        
        # NaN ê°’ ì œê±°
        df = df.dropna(subset=['body', 'label_id'])
        
        print(f"ë¡œë“œëœ ë°ì´í„°: {len(df)}ê°œ")
        print(f"ë¼ë²¨ ë¶„í¬:\n{df['label'].value_counts()}")
        
        return df
    
    def predict_single(self, text):
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            predicted_label: ì˜ˆì¸¡ëœ ë¼ë²¨
            confidence: í™•ì‹ ë„
        """
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True
        )
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1)
            predicted_id = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_id].item()
        
        predicted_label = self.id2label[predicted_id]
        
        return predicted_label, confidence
    
    def evaluate_batch(self, df, batch_size=16):
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ í‰ê°€
        
        Args:
            df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            confidences: í™•ì‹ ë„ ë¦¬ìŠ¤íŠ¸
        """
        predictions = []
        confidences = []
        
        print("ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
        
        for i in tqdm(range(0, len(df), batch_size)):
            batch_df = df.iloc[i:i+batch_size]
            batch_texts = batch_df['body'].tolist()
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                batch_texts,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            
            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.softmax(outputs.logits, dim=-1)
                predicted_ids = torch.argmax(probabilities, dim=-1)
                batch_confidences = torch.max(probabilities, dim=-1)[0]
            
            # ê²°ê³¼ ë³€í™˜
            batch_predictions = [self.id2label[id.item()] for id in predicted_ids]
            batch_confidences = batch_confidences.cpu().numpy()
            
            predictions.extend(batch_predictions)
            confidences.extend(batch_confidences)
        
        return predictions, confidences
    
    def calculate_metrics(self, true_labels, predicted_labels):
        """
        ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            true_labels: ì‹¤ì œ ë¼ë²¨
            predicted_labels: ì˜ˆì¸¡ ë¼ë²¨
            
        Returns:
            metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        accuracy = accuracy_score(true_labels, predicted_labels)
        f1_macro = f1_score(true_labels, predicted_labels, average='macro')
        f1_weighted = f1_score(true_labels, predicted_labels, average='weighted')
        
        # í´ë˜ìŠ¤ë³„ F1-Score
        f1_per_class = f1_score(true_labels, predicted_labels, average=None)
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        class_report = classification_report(
            true_labels, 
            predicted_labels, 
            target_names=list(self.label2id.keys()),
            output_dict=True
        )
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(true_labels, predicted_labels)
        
        metrics = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'f1_per_class': f1_per_class,
            'classification_report': class_report,
            'confusion_matrix': cm
        }
        
        return metrics
    
    def plot_confusion_matrix(self, confusion_matrix, save_path=None):
        """
        í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        
        Args:
            confusion_matrix: í˜¼ë™ í–‰ë ¬
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ í‘œì‹œë§Œ)
        """
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            confusion_matrix, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=list(self.label2id.keys()),
            yticklabels=list(self.label2id.keys())
        )
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"í˜¼ë™ í–‰ë ¬ì´ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        plt.show()
    
    def print_results(self, metrics, model_name):
        """
        ê²°ê³¼ ì¶œë ¥
        
        Args:
            metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­
            model_name: ëª¨ë¸ ì´ë¦„
        """
        print("\n" + "="*60)
        print(f"{model_name} ëª¨ë¸ - test_labeled_dataset.csv ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("="*60)
        
        print(f"\nğŸ“Š ì „ì²´ ì„±ëŠ¥:")
        print(f"   ì •í™•ë„ (Accuracy): {metrics['accuracy']:.4f}")
        print(f"   F1-Score (Macro): {metrics['f1_macro']:.4f}")
        print(f"   F1-Score (Weighted): {metrics['f1_weighted']:.4f}")
        
        print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ F1-Score:")
        for i, label in enumerate(self.label2id.keys()):
            print(f"   {label}: {metrics['f1_per_class'][i]:.4f}")
        
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        try:
            # ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ ì‚¬ìš©í•´ì„œ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„±
            print(classification_report(
                list(self.label2id.keys()),
                list(self.label2id.keys()),  # ì„ì‹œë¡œ ê°™ì€ ê°’ ì‚¬ìš©
                target_names=list(self.label2id.keys())
            ))
        except:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê°„ë‹¨í•œ ë¦¬í¬íŠ¸ ì¶œë ¥
            print("ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("í˜¼ë™ í–‰ë ¬ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # í˜¼ë™ í–‰ë ¬ ì¶œë ¥
        print(f"\nğŸ” í˜¼ë™ í–‰ë ¬:")
        cm = metrics['confusion_matrix']
        print("ì‹¤ì œ\\ì˜ˆì¸¡\tWork\tPersonal\tAdvertisement")
        for i, true_label in enumerate(self.label2id.keys()):
            print(f"{true_label}\t\t{cm[i][0]}\t{cm[i][1]}\t\t{cm[i][2]}")
    
    def save_results(self, metrics, predictions, confidences, save_path):
        """
        ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­
            predictions: ì˜ˆì¸¡ ê²°ê³¼
            confidences: í™•ì‹ ë„
            save_path: ì €ì¥ ê²½ë¡œ
        """
        results = {
            'model_path': self.model_path,
            'test_dataset': self.test_data_path,
            'total_samples': len(predictions),
            'metrics': {
                'accuracy': float(metrics['accuracy']),
                'f1_macro': float(metrics['f1_macro']),
                'f1_weighted': float(metrics['f1_weighted']),
                'f1_per_class': metrics['f1_per_class'].tolist()
            },
            'predictions': predictions,
            'confidences': [float(c) for c in confidences]
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nê²°ê³¼ê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def run_evaluation(self, batch_size=16, save_results=True):
        """
        ì „ì²´ í‰ê°€ ì‹¤í–‰
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
        """
        print(f"{self.model_path} ëª¨ë¸ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_df = self.load_test_data()
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions, confidences = self.evaluate_batch(test_df, batch_size=batch_size)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        true_labels = test_df['label'].tolist()
        metrics = self.calculate_metrics(true_labels, predictions)
        
        # ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
        model_name = os.path.basename(self.model_path)
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_results(metrics, model_name)
        
        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        self.plot_confusion_matrix(metrics['confusion_matrix'])
        
        # ê²°ê³¼ ì €ì¥
        if save_results:
            results_path = os.path.join(os.path.dirname(self.model_path), f'test_dataset_evaluation_{model_name}.json')
            self.save_results(metrics, predictions, confidences, results_path)
        
        # í‰ê·  í™•ì‹ ë„ ê³„ì‚°
        avg_confidence = np.mean(confidences)
        print(f"\nğŸ“Š í‰ê·  í™•ì‹ ë„: {avg_confidence:.4f}")
        
        return metrics, predictions, confidences

def evaluate_all_models(test_data_path="dataset/test_labeled_dataset.csv"):
    """ëª¨ë“  ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    
    # í‰ê°€í•  ëª¨ë¸ë“¤
    models = [
        "models/kobert_trained/checkpoint-375",
        "models/kobert_trained_improved/checkpoint-500", 
        "bert_manual_300_model/checkpoint-264"
    ]
    
    results = {}
    
    for model_path in models:
        if os.path.exists(model_path):
            try:
                print(f"\n{'='*80}")
                print(f"ëª¨ë¸ í‰ê°€: {model_path}")
                print(f"{'='*80}")
                
                evaluator = TestDatasetEvaluator(model_path, test_data_path)
                metrics, predictions, confidences = evaluator.run_evaluation(
                    batch_size=16,
                    save_results=True
                )
                
                results[model_path] = {
                    'accuracy': metrics['accuracy'],
                    'f1_macro': metrics['f1_macro'],
                    'f1_weighted': metrics['f1_weighted'],
                    'avg_confidence': np.mean(confidences)
                }
                
            except Exception as e:
                print(f"ëª¨ë¸ {model_path} í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue
        else:
            print(f"ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
    
    # ê²°ê³¼ ë¹„êµ
    print(f"\n{'='*80}")
    print("ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    print(f"{'='*80}")
    
    if results:
        comparison_df = pd.DataFrame(results).T
        comparison_df = comparison_df.sort_values('accuracy', ascending=False)
        
        print("\nğŸ“Š ì„±ëŠ¥ ìˆœìœ„ (ì •í™•ë„ ê¸°ì¤€):")
        for i, (model, metrics) in enumerate(comparison_df.iterrows(), 1):
            model_name = os.path.basename(model)
            print(f"{i}. {model_name}")
            print(f"   ì •í™•ë„: {metrics['accuracy']:.4f}")
            print(f"   F1-Macro: {metrics['f1_macro']:.4f}")
            print(f"   F1-Weighted: {metrics['f1_weighted']:.4f}")
            print(f"   í‰ê·  í™•ì‹ ë„: {metrics['avg_confidence']:.4f}")
            print()
    
    return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    if len(sys.argv) > 1:
        test_data_path = sys.argv[1]
    else:
        test_data_path = "dataset/test_labeled_dataset.csv"
    print(f"{test_data_path}ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ëª¨ë“  ëª¨ë¸ í‰ê°€
    results = evaluate_all_models(test_data_path)
    
    print("\nâœ… ëª¨ë“  ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 