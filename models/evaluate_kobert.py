import torch
import pandas as pd
import numpy as np
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import os
import json

class KoBERTEvaluator:
    def __init__(self, model_path, test_data_path):
        """
        KoBERT ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”
        
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
        """
        self.model_path = model_path
        self.test_data_path = test_data_path
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path, trust_remote_code=True)
        # ì›ë³¸ KoBERT í† í¬ë‚˜ì´ì € ì‚¬ìš© (ì €ì¥ëœ í† í¬ë‚˜ì´ì €ì— ë¬¸ì œê°€ ìˆìŒ)
        self.tokenizer = AutoTokenizer.from_pretrained('monologg/kobert', trust_remote_code=True)
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()
        self.model.to(self.device)
        
        # ë¼ë²¨ ë§¤í•‘ ë¡œë“œ
        self.label2id = {"Work": 0, "Advertisement": 1, "Personal": 2}
        self.id2label = {0: "Work", 1: "Advertisement", 2: "Personal"}
        
        print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    def load_test_data(self, sample_size=None):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        
        Args:
            sample_size: ìƒ˜í”Œ í¬ê¸° (Noneì´ë©´ ì „ì²´ ë°ì´í„°)
        """
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤... ({self.test_data_path})")
        
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(self.test_data_path)
        
        # ìƒ˜í”Œë§ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        if sample_size and sample_size < len(df):
            df = df.sample(n=sample_size, random_state=42)
            print(f"ë°ì´í„° ìƒ˜í”Œë§: {sample_size}ê°œ ì‚¬ìš©")
        
        # ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
        df['label_id'] = df['label'].map(self.label2id)
        
        # NaN ê°’ ì œê±°
        df = df.dropna(subset=['text', 'label_id'])
        
        print(f"ë¡œë“œëœ ë°ì´í„°: {len(df)}ê°œ")
        print(f"ë¼ë²¨ ë¶„í¬:\n{df['label'].value_counts()}")
        
        return df
    
    def predict_single(self, text):
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            predicted_label: ì˜ˆì¸¡ëœ ë¼ë²¨
            confidence: í™•ì‹ ë„
        """
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True
        )
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1)
            predicted_id = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_id].item()
        
        predicted_label = self.id2label[predicted_id]
        
        return predicted_label, confidence
    
    def evaluate_batch(self, df, batch_size=32):
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ í‰ê°€
        
        Args:
            df: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            confidences: í™•ì‹ ë„ ë¦¬ìŠ¤íŠ¸
        """
        predictions = []
        confidences = []
        
        print("ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
        
        for i in tqdm(range(0, len(df), batch_size)):
            batch_df = df.iloc[i:i+batch_size]
            batch_texts = batch_df['text'].tolist()
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                batch_texts,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            
            # GPUë¡œ ì´ë™
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.softmax(outputs.logits, dim=-1)
                predicted_ids = torch.argmax(probabilities, dim=-1)
                batch_confidences = torch.max(probabilities, dim=-1)[0]
            
            # ê²°ê³¼ ë³€í™˜
            batch_predictions = [self.id2label[id.item()] for id in predicted_ids]
            batch_confidences = batch_confidences.cpu().numpy()
            
            predictions.extend(batch_predictions)
            confidences.extend(batch_confidences)
        
        return predictions, confidences
    
    def calculate_metrics(self, true_labels, predicted_labels):
        """
        ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            true_labels: ì‹¤ì œ ë¼ë²¨
            predicted_labels: ì˜ˆì¸¡ ë¼ë²¨
            
        Returns:
            metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        accuracy = accuracy_score(true_labels, predicted_labels)
        f1_macro = f1_score(true_labels, predicted_labels, average='macro')
        f1_weighted = f1_score(true_labels, predicted_labels, average='weighted')
        
        # í´ë˜ìŠ¤ë³„ F1-Score
        f1_per_class = f1_score(true_labels, predicted_labels, average=None)
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        class_report = classification_report(
            true_labels, 
            predicted_labels, 
            target_names=list(self.label2id.keys()),
            output_dict=True
        )
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(true_labels, predicted_labels)
        
        metrics = {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'f1_per_class': f1_per_class,
            'classification_report': class_report,
            'confusion_matrix': cm
        }
        
        return metrics
    
    def plot_confusion_matrix(self, confusion_matrix, save_path=None):
        """
        í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        
        Args:
            confusion_matrix: í˜¼ë™ í–‰ë ¬
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ í‘œì‹œë§Œ)
        """
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            confusion_matrix, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=list(self.label2id.keys()),
            yticklabels=list(self.label2id.keys())
        )
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"í˜¼ë™ í–‰ë ¬ì´ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        plt.show()
    
    def print_results(self, metrics):
        """
        ê²°ê³¼ ì¶œë ¥
        
        Args:
            metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­
        """
        print("\n" + "="*50)
        print("KoBERT ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
        print("="*50)
        
        print(f"\nğŸ“Š ì „ì²´ ì„±ëŠ¥:")
        print(f"   ì •í™•ë„ (Accuracy): {metrics['accuracy']:.4f}")
        print(f"   F1-Score (Macro): {metrics['f1_macro']:.4f}")
        print(f"   F1-Score (Weighted): {metrics['f1_weighted']:.4f}")
        
        print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ F1-Score:")
        for i, label in enumerate(self.label2id.keys()):
            print(f"   {label}: {metrics['f1_per_class'][i]:.4f}")
        
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        # classification_report ì—ëŸ¬ ìˆ˜ì •
        try:
            print(classification_report(
                list(self.label2id.keys()),
                metrics['classification_report'],
                target_names=list(self.label2id.keys())
            ))
        except:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê°„ë‹¨í•œ ë¦¬í¬íŠ¸ ì¶œë ¥
            print("ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("í˜¼ë™ í–‰ë ¬ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # í˜¼ë™ í–‰ë ¬ ì¶œë ¥
        print(f"\nğŸ” í˜¼ë™ í–‰ë ¬:")
        cm = metrics['confusion_matrix']
        print("ì‹¤ì œ\ì˜ˆì¸¡\tWork\tAdvertisement\tPersonal")
        for i, true_label in enumerate(self.label2id.keys()):
            print(f"{true_label}\t\t{cm[i][0]}\t{cm[i][1]}\t\t{cm[i][2]}")
        
        # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„
        print(f"\nğŸ“Š ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„:")
        unique, counts = np.unique(list(self.label2id.keys()), return_counts=True)
        for label, count in zip(unique, counts):
            print(f"   {label}: {count}ê°œ ì˜ˆì¸¡")
    
    def save_results(self, metrics, predictions, confidences, save_path):
        """
        ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­
            predictions: ì˜ˆì¸¡ ê²°ê³¼
            confidences: í™•ì‹ ë„
            save_path: ì €ì¥ ê²½ë¡œ
        """
        results = {
            'metrics': {
                'accuracy': float(metrics['accuracy']),
                'f1_macro': float(metrics['f1_macro']),
                'f1_weighted': float(metrics['f1_weighted']),
                'f1_per_class': metrics['f1_per_class'].tolist()
            },
            'predictions': predictions,
            'confidences': [float(c) for c in confidences]
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nê²°ê³¼ê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def run_evaluation(self, sample_size=1000, batch_size=32, save_results=True):
        """
        ì „ì²´ í‰ê°€ ì‹¤í–‰
        
        Args:
            sample_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œ í¬ê¸°
            batch_size: ë°°ì¹˜ í¬ê¸°
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
        """
        print("KoBERT ëª¨ë¸ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_df = self.load_test_data(sample_size=sample_size)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions, confidences = self.evaluate_batch(test_df, batch_size=batch_size)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        true_labels = test_df['label'].tolist()
        metrics = self.calculate_metrics(true_labels, predictions)
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_results(metrics)
        
        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        self.plot_confusion_matrix(metrics['confusion_matrix'])
        
        # ê²°ê³¼ ì €ì¥
        if save_results:
            results_path = os.path.join(os.path.dirname(self.model_path), 'evaluation_results.json')
            self.save_results(metrics, predictions, confidences, results_path)
        
        # í‰ê·  í™•ì‹ ë„ ê³„ì‚°
        avg_confidence = np.mean(confidences)
        print(f"\nğŸ“Š í‰ê·  í™•ì‹ ë„: {avg_confidence:.4f}")
        
        return metrics, predictions, confidences

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì„¤ì •
    model_path = "models/kobert_trained/checkpoint-375"
    test_data_path = "data/processed/validation.csv"
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = KoBERTEvaluator(model_path, test_data_path)
    
    # í‰ê°€ ì‹¤í–‰ (1000ê°œ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸)
    metrics, predictions, confidences = evaluator.run_evaluation(
        sample_size=1000,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ìƒ˜í”Œë§
        batch_size=16,     # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
        save_results=True
    )
    
    print("\nâœ… í‰ê°€ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 