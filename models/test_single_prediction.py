import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

def test_single_prediction():
    """ë‹¨ì¼ ì´ë©”ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_path = "models/kobert_trained/checkpoint-375"
    print(f"ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {model_path}")
    
    model = AutoModelForSequenceClassification.from_pretrained(model_path, trust_remote_code=True)
    # ì›ë³¸ KoBERT í† í¬ë‚˜ì´ì € ì‚¬ìš© (ì €ì¥ëœ í† í¬ë‚˜ì´ì €ì— ë¬¸ì œê°€ ìˆìŒ)
    tokenizer = AutoTokenizer.from_pretrained('monologg/kobert', trust_remote_code=True)
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    model.eval()
    model.to(device)
    
    # ë¼ë²¨ ë§¤í•‘
    id2label = {0: "Work", 1: "Advertisement", 2: "Personal"}
    
    # í…ŒìŠ¤íŠ¸í•  ì´ë©”ì¼ í…ìŠ¤íŠ¸ë“¤
    test_emails = [
        "ë‚´ì¼ ì˜¤í›„ 2ì‹œì— í”„ë¡œì íŠ¸ íšŒì˜ê°€ ìˆìŠµë‹ˆë‹¤. ì°¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
        "ì£¼ë§ì— ê°™ì´ ì˜í™” ë³´ëŸ¬ ê°ˆê¹Œìš”? ì¬ë¯¸ìˆëŠ” ì˜í™”ê°€ ê°œë´‰í–ˆëŒ€ìš”.",
        "í•œì • ì‹œê°„ íŠ¹ê°€! ì§€ê¸ˆ êµ¬ë§¤í•˜ì‹œë©´ 50% í• ì¸! ì„œë‘ë¥´ì„¸ìš”!",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”. ì €ë…ì— ê°™ì´ ë°¥ ë¨¹ì„ê¹Œìš”?",
        "ë‹¤ìŒ ì£¼ ì›”ìš”ì¼ê¹Œì§€ ë³´ê³ ì„œ ì œì¶œí•´ì£¼ì„¸ìš”. ê¸‰í•©ë‹ˆë‹¤.",
        "ìƒˆë¡œìš´ ì œí’ˆ ì¶œì‹œ ì•ˆë‚´ì…ë‹ˆë‹¤. ì§€ê¸ˆ êµ¬ë§¤í•˜ì‹œë©´ ì‚¬ì€í’ˆ ì¦ì •!"
    ]
    
    print("\n" + "="*60)
    print("KoBERT ëª¨ë¸ ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    for i, email_text in enumerate(test_emails, 1):
        print(f"\nğŸ“§ í…ŒìŠ¤íŠ¸ ì´ë©”ì¼ {i}:")
        print(f"   í…ìŠ¤íŠ¸: {email_text}")
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            email_text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True
        )
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1)
            predicted_id = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_id].item()
        
        predicted_label = id2label[predicted_id]
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"   ì˜ˆì¸¡ ê²°ê³¼: {predicted_label}")
        print(f"   í™•ì‹ ë„: {confidence:.4f}")
        
        # ëª¨ë“  í´ë˜ìŠ¤ë³„ í™•ë¥ 
        print(f"   í´ë˜ìŠ¤ë³„ í™•ë¥ :")
        for class_id, class_name in id2label.items():
            prob = probabilities[0][class_id].item()
            print(f"     {class_name}: {prob:.4f}")
        
        print("-" * 40)

if __name__ == "__main__":
    test_single_prediction() 