import json
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import os

def get_detailed_metrics():
    """ëª¨ë“  ëª¨ë¸ì˜ ìƒì„¸í•œ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    
    print("="*80)
    print("ëª¨ë“  ëª¨ë¸ ìƒì„¸ ì„±ëŠ¥ ì§€í‘œ (F1, Recall, Accuracy, Precision)")
    print("="*80)
    
    # í‰ê°€ ê²°ê³¼ íŒŒì¼ë“¤
    evaluation_files = [
        "bert_manual_300_model/test_dataset_evaluation_checkpoint-264.json",
        "models/kobert_trained/test_dataset_evaluation_checkpoint-375.json",
        "models/kobert_trained_improved/test_dataset_evaluation_checkpoint-500.json"
    ]
    
    results_summary = []
    
    for file_path in evaluation_files:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                model_name = os.path.basename(data['model_path'])
                test_dataset = os.path.basename(data['test_dataset'])
                
                print(f"\n{'='*60}")
                print(f"ëª¨ë¸: {model_name}")
                print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹: {test_dataset}")
                print(f"ì´ ìƒ˜í”Œ ìˆ˜: {data['total_samples']}")
                print(f"{'='*60}")
                
                # ê¸°ë³¸ ë©”íŠ¸ë¦­
                accuracy = data['metrics']['accuracy']
                f1_macro = data['metrics']['f1_macro']
                f1_weighted = data['metrics']['f1_weighted']
                f1_per_class = data['metrics']['f1_per_class']
                
                print(f"\nğŸ“Š ì „ì²´ ì„±ëŠ¥:")
                print(f"   ì •í™•ë„ (Accuracy): {accuracy:.4f}")
                print(f"   F1-Score (Macro): {f1_macro:.4f}")
                print(f"   F1-Score (Weighted): {f1_weighted:.4f}")
                
                # í´ë˜ìŠ¤ë³„ ìƒì„¸ ì§€í‘œ ê³„ì‚°
                predictions = data['predictions']
                confidences = data['confidences']
                
                # ì‹¤ì œ ë¼ë²¨ ì¶”ì¶œ (ì˜¤ë²„ìƒ˜í”Œë§ëœ ë°ì´í„°ì˜ ê²½ìš°)
                if test_dataset == "test_labeled_dataset_oversampled.csv":
                    # ì˜¤ë²„ìƒ˜í”Œë§ëœ ë°ì´í„°ì—ì„œ ì‹¤ì œ ë¼ë²¨ ì¶”ì¶œ
                    df = pd.read_csv('dataset/test_labeled_dataset_oversampled.csv')
                    true_labels = df['label'].tolist()
                else:
                    # ì›ë³¸ ë°ì´í„°ì—ì„œ ì‹¤ì œ ë¼ë²¨ ì¶”ì¶œ
                    df = pd.read_csv('dataset/test_labeled_dataset.csv')
                    true_labels = df['label'].tolist()
                
                # í´ë˜ìŠ¤ë³„ ìƒì„¸ ì§€í‘œ ê³„ì‚°
                labels = ["Work", "Personal", "Advertisement"]
                precision, recall, f1, support = precision_recall_fscore_support(
                    true_labels, predictions, labels=labels, average=None
                )
                
                print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ìƒì„¸ ì§€í‘œ:")
                print(f"{'í´ë˜ìŠ¤':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
                print("-" * 55)
                
                for i, label in enumerate(labels):
                    print(f"{label:<12} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f} {support[i]:<10.0f}")
                
                # ì „ì²´ í‰ê·  ì§€í‘œ
                precision_macro, recall_macro, f1_macro_calc, _ = precision_recall_fscore_support(
                    true_labels, predictions, average='macro'
                )
                precision_weighted, recall_weighted, f1_weighted_calc, _ = precision_recall_fscore_support(
                    true_labels, predictions, average='weighted'
                )
                
                print(f"\nğŸ“Š ì „ì²´ í‰ê·  ì§€í‘œ:")
                print(f"   Precision (Macro): {precision_macro:.4f}")
                print(f"   Recall (Macro): {recall_macro:.4f}")
                print(f"   F1-Score (Macro): {f1_macro_calc:.4f}")
                print(f"   Precision (Weighted): {precision_weighted:.4f}")
                print(f"   Recall (Weighted): {recall_weighted:.4f}")
                print(f"   F1-Score (Weighted): {f1_weighted_calc:.4f}")
                
                # í‰ê·  í™•ì‹ ë„
                avg_confidence = np.mean(confidences)
                print(f"   í‰ê·  í™•ì‹ ë„: {avg_confidence:.4f}")
                
                # ê²°ê³¼ ìš”ì•½ ì €ì¥
                results_summary.append({
                    'model': model_name,
                    'dataset': test_dataset,
                    'accuracy': accuracy,
                    'precision_macro': precision_macro,
                    'recall_macro': recall_macro,
                    'f1_macro': f1_macro_calc,
                    'precision_weighted': precision_weighted,
                    'recall_weighted': recall_weighted,
                    'f1_weighted': f1_weighted_calc,
                    'avg_confidence': avg_confidence,
                    'total_samples': data['total_samples']
                })
                
                # í˜¼ë™ í–‰ë ¬ ì¶œë ¥
                print(f"\nğŸ” í˜¼ë™ í–‰ë ¬:")
                from sklearn.metrics import confusion_matrix
                cm = confusion_matrix(true_labels, predictions, labels=labels)
                print("ì‹¤ì œ\\ì˜ˆì¸¡\tWork\tPersonal\tAdvertisement")
                for i, true_label in enumerate(labels):
                    print(f"{true_label}\t\t{cm[i][0]}\t{cm[i][1]}\t\t{cm[i][2]}")
                
            except Exception as e:
                print(f"íŒŒì¼ {file_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            print(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
    
    # ëª¨ë“  ëª¨ë¸ ë¹„êµ
    if results_summary:
        print(f"\n{'='*80}")
        print("ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print(f"{'='*80}")
        
        df_summary = pd.DataFrame(results_summary)
        
        # ì •í™•ë„ ê¸°ì¤€ ì •ë ¬
        df_summary_sorted = df_summary.sort_values('accuracy', ascending=False)
        
        print(f"\nğŸ“Š ì„±ëŠ¥ ìˆœìœ„ (ì •í™•ë„ ê¸°ì¤€):")
        print(f"{'ìˆœìœ„':<4} {'ëª¨ë¸':<25} {'ë°ì´í„°ì…‹':<30} {'Accuracy':<10} {'F1-Macro':<10} {'Recall-Macro':<12} {'Precision-Macro':<15}")
        print("-" * 120)
        
        for i, (_, row) in enumerate(df_summary_sorted.iterrows(), 1):
            print(f"{i:<4} {row['model']:<25} {row['dataset']:<30} {row['accuracy']:<10.4f} {row['f1_macro']:<10.4f} {row['recall_macro']:<12.4f} {row['precision_macro']:<15.4f}")
        
        # ìƒì„¸ ë¹„êµí‘œ
        print(f"\nğŸ“‹ ìƒì„¸ ë¹„êµí‘œ:")
        print(df_summary_sorted[['model', 'dataset', 'accuracy', 'precision_macro', 'recall_macro', 'f1_macro', 'avg_confidence']].to_string(index=False))
        
        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        output_file = "model_performance_comparison.csv"
        df_summary_sorted.to_csv(output_file, index=False, encoding='utf-8')
        print(f"\nâœ… ìƒì„¸ ë¹„êµ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return df_summary_sorted
    
    return None

def analyze_best_model():
    """ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì˜ ìƒì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    
    print(f"\n{'='*80}")
    print("ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìƒì„¸ ë¶„ì„")
    print(f"{'='*80}")
    
    # ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ ì°¾ê¸°
    evaluation_files = [
        "bert_manual_300_model/test_dataset_evaluation_checkpoint-264.json",
        "models/kobert_trained/test_dataset_evaluation_checkpoint-375.json",
        "models/kobert_trained_improved/test_dataset_evaluation_checkpoint-500.json"
    ]
    
    best_model = None
    best_accuracy = 0
    
    for file_path in evaluation_files:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            accuracy = data['metrics']['accuracy']
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_model = data
    
    if best_model:
        print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_path']}")
        print(f"ì •í™•ë„: {best_model['metrics']['accuracy']:.4f}")
        print(f"F1-Macro: {best_model['metrics']['f1_macro']:.4f}")
        
        # ìƒì„¸ ë¶„ì„
        predictions = best_model['predictions']
        confidences = best_model['confidences']
        
        # ì‹¤ì œ ë¼ë²¨ ì¶”ì¶œ
        test_dataset = os.path.basename(best_model['test_dataset'])
        if test_dataset == "test_labeled_dataset_oversampled.csv":
            df = pd.read_csv('dataset/test_labeled_dataset_oversampled.csv')
        else:
            df = pd.read_csv('dataset/test_labeled_dataset.csv')
        
        true_labels = df['label'].tolist()
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(classification_report(true_labels, predictions, target_names=["Work", "Personal", "Advertisement"]))
        
        # í™•ì‹ ë„ ë¶„ì„
        print(f"\nğŸ“Š í™•ì‹ ë„ ë¶„ì„:")
        print(f"í‰ê·  í™•ì‹ ë„: {np.mean(confidences):.4f}")
        print(f"ìµœì†Œ í™•ì‹ ë„: {np.min(confidences):.4f}")
        print(f"ìµœëŒ€ í™•ì‹ ë„: {np.max(confidences):.4f}")
        
        # ë¼ë²¨ë³„ í™•ì‹ ë„
        df_analysis = pd.DataFrame({
            'true_label': true_labels,
            'predicted_label': predictions,
            'confidence': confidences,
            'correct': [t == p for t, p in zip(true_labels, predictions)]
        })
        
        print(f"\në¼ë²¨ë³„ í‰ê·  í™•ì‹ ë„:")
        for label in df_analysis['true_label'].unique():
            label_data = df_analysis[df_analysis['true_label'] == label]
            avg_conf = label_data['confidence'].mean()
            accuracy = label_data['correct'].mean()
            print(f"{label}: í™•ì‹ ë„ {avg_conf:.4f}, ì •í™•ë„ {accuracy:.4f}")

if __name__ == "__main__":
    # ìƒì„¸ ì§€í‘œ ê³„ì‚°
    results = get_detailed_metrics()
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¶„ì„
    analyze_best_model() 